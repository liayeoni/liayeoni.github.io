---
title: Kaggle Titanic Disaster Dataset
description: 
author: Ahyeon Lee
date: 2024-04-07
categories: [Kaggle]
tags: [Kaggle]
pin: true
math: true
mermaid: true
---

```python
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
plt.style.use('fivethirtyeight')
import warnings
warnings.filterwarnings('ignore')
%matplotlib inline
```

### 01. 데이터 확인


```python
df = pd.read_csv('./train.csv')
df.head()
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>PassengerId</th>
      <th>Survived</th>
      <th>Pclass</th>
      <th>Name</th>
      <th>Sex</th>
      <th>Age</th>
      <th>SibSp</th>
      <th>Parch</th>
      <th>Ticket</th>
      <th>Fare</th>
      <th>Cabin</th>
      <th>Embarked</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>1</td>
      <td>0</td>
      <td>3</td>
      <td>Braund, Mr. Owen Harris</td>
      <td>male</td>
      <td>22.0</td>
      <td>1</td>
      <td>0</td>
      <td>A/5 21171</td>
      <td>7.2500</td>
      <td>NaN</td>
      <td>S</td>
    </tr>
    <tr>
      <th>1</th>
      <td>2</td>
      <td>1</td>
      <td>1</td>
      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>
      <td>female</td>
      <td>38.0</td>
      <td>1</td>
      <td>0</td>
      <td>PC 17599</td>
      <td>71.2833</td>
      <td>C85</td>
      <td>C</td>
    </tr>
    <tr>
      <th>2</th>
      <td>3</td>
      <td>1</td>
      <td>3</td>
      <td>Heikkinen, Miss. Laina</td>
      <td>female</td>
      <td>26.0</td>
      <td>0</td>
      <td>0</td>
      <td>STON/O2. 3101282</td>
      <td>7.9250</td>
      <td>NaN</td>
      <td>S</td>
    </tr>
    <tr>
      <th>3</th>
      <td>4</td>
      <td>1</td>
      <td>1</td>
      <td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>
      <td>female</td>
      <td>35.0</td>
      <td>1</td>
      <td>0</td>
      <td>113803</td>
      <td>53.1000</td>
      <td>C123</td>
      <td>S</td>
    </tr>
    <tr>
      <th>4</th>
      <td>5</td>
      <td>0</td>
      <td>3</td>
      <td>Allen, Mr. William Henry</td>
      <td>male</td>
      <td>35.0</td>
      <td>0</td>
      <td>0</td>
      <td>373450</td>
      <td>8.0500</td>
      <td>NaN</td>
      <td>S</td>
    </tr>
  </tbody>
</table>
</div>




```python
df_test = pd.read_csv('./test.csv')
df_test.head()
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>PassengerId</th>
      <th>Pclass</th>
      <th>Name</th>
      <th>Sex</th>
      <th>Age</th>
      <th>SibSp</th>
      <th>Parch</th>
      <th>Ticket</th>
      <th>Fare</th>
      <th>Cabin</th>
      <th>Embarked</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>892</td>
      <td>3</td>
      <td>Kelly, Mr. James</td>
      <td>male</td>
      <td>34.5</td>
      <td>0</td>
      <td>0</td>
      <td>330911</td>
      <td>7.8292</td>
      <td>NaN</td>
      <td>Q</td>
    </tr>
    <tr>
      <th>1</th>
      <td>893</td>
      <td>3</td>
      <td>Wilkes, Mrs. James (Ellen Needs)</td>
      <td>female</td>
      <td>47.0</td>
      <td>1</td>
      <td>0</td>
      <td>363272</td>
      <td>7.0000</td>
      <td>NaN</td>
      <td>S</td>
    </tr>
    <tr>
      <th>2</th>
      <td>894</td>
      <td>2</td>
      <td>Myles, Mr. Thomas Francis</td>
      <td>male</td>
      <td>62.0</td>
      <td>0</td>
      <td>0</td>
      <td>240276</td>
      <td>9.6875</td>
      <td>NaN</td>
      <td>Q</td>
    </tr>
    <tr>
      <th>3</th>
      <td>895</td>
      <td>3</td>
      <td>Wirz, Mr. Albert</td>
      <td>male</td>
      <td>27.0</td>
      <td>0</td>
      <td>0</td>
      <td>315154</td>
      <td>8.6625</td>
      <td>NaN</td>
      <td>S</td>
    </tr>
    <tr>
      <th>4</th>
      <td>896</td>
      <td>3</td>
      <td>Hirvonen, Mrs. Alexander (Helga E Lindqvist)</td>
      <td>female</td>
      <td>22.0</td>
      <td>1</td>
      <td>1</td>
      <td>3101298</td>
      <td>12.2875</td>
      <td>NaN</td>
      <td>S</td>
    </tr>
  </tbody>
</table>
</div>




```python
df.info()
```

    <class 'pandas.core.frame.DataFrame'>
    RangeIndex: 891 entries, 0 to 890
    Data columns (total 12 columns):
     #   Column       Non-Null Count  Dtype  
    ---  ------       --------------  -----  
     0   PassengerId  891 non-null    int64  
     1   Survived     891 non-null    int64  
     2   Pclass       891 non-null    int64  
     3   Name         891 non-null    object 
     4   Sex          891 non-null    object 
     5   Age          714 non-null    float64
     6   SibSp        891 non-null    int64  
     7   Parch        891 non-null    int64  
     8   Ticket       891 non-null    object 
     9   Fare         891 non-null    float64
     10  Cabin        204 non-null    object 
     11  Embarked     889 non-null    object 
    dtypes: float64(2), int64(5), object(5)
    memory usage: 83.7+ KB
    


```python
df.shape
```




    (891, 12)




```python
df.isnull().sum()
```




    PassengerId      0
    Survived         0
    Pclass           0
    Name             0
    Sex              0
    Age            177
    SibSp            0
    Parch            0
    Ticket           0
    Fare             0
    Cabin          687
    Embarked         2
    dtype: int64




```python
df_test.isnull().sum()
```




    PassengerId      0
    Pclass           0
    Name             0
    Sex              0
    Age             86
    SibSp            0
    Parch            0
    Ticket           0
    Fare             1
    Cabin          327
    Embarked         0
    dtype: int64




```python
# 컬럼별 null data % 출력
# {:>10} 오른쪽 정렬
for col in df.columns:
    msg = 'column : {:>10}\t Percent of Nan value : {:2f}%'.format(col, (df[col].isnull().sum() / df[col].shape[0])* 100)
    print(msg)
```

    column : PassengerId	 Percent of Nan value : 0.000000%
    column :   Survived	 Percent of Nan value : 0.000000%
    column :     Pclass	 Percent of Nan value : 0.000000%
    column :       Name	 Percent of Nan value : 0.000000%
    column :        Sex	 Percent of Nan value : 0.000000%
    column :        Age	 Percent of Nan value : 19.865320%
    column :      SibSp	 Percent of Nan value : 0.000000%
    column :      Parch	 Percent of Nan value : 0.000000%
    column :     Ticket	 Percent of Nan value : 0.000000%
    column :       Fare	 Percent of Nan value : 0.000000%
    column :      Cabin	 Percent of Nan value : 77.104377%
    column :   Embarked	 Percent of Nan value : 0.224467%
    

- Age, Cabin 컬럼에서 Null값이 많다.

### 02. 전체 생존확률 확인


```python
f, ax = plt.subplots(1, 2, figsize=(18, 8))
df['Survived'].value_counts().plot.pie(explode=[0, 0.1], autopct ='%1.1f%%', ax=ax[0], shadow=True)
ax[0].set_title('Survived')
ax[0].set_ylabel('')
sns.countplot(x='Survived', data=df, ax=ax[1])
ax[1].set_title('Survived')
```




    Text(0.5, 1.0, 'Survived')




    
![image](https://github.com/liayeoni/study/assets/154586550/549f82d4-79d4-4bba-83e1-439ce9f51ab0)
    


- 생존한 사람 38.4%, 생존하지 못한 사람 61.6%로 생존하지 못한 사람의 비율이 더 높다.


```python
df['Survived'].value_counts()
```




    Survived
    0    549
    1    342
    Name: count, dtype: int64



### 0.3 각 컬럼별 생존 비율 확인하기

- Type of Features
    - Categorical Features : 두 개 이상의 범주를 가지며 해당 특성의 값이 그 범주로 분류될 수 있는 변수를 의미, 순서가 정해져있지 않다.
        eg. Sex, Embarked
    - Ordinal Features : 상대적인 순서나 정렬을 가질 수 있다. eg. Pclass
    - Continuous Feature : 해당 특성의 열에서 최솟값과 최댓값 사이, 어떤 두 점 사이 값들을 가질 수 있는 경우
        eg. Age

#### 03-01. 성별에 따른 생존 비율 확인(Categorical Feature)
- Sex로 groupby된 Sex, Survived 데이터로 막대 그래프 그리기
- 성별 Survived여부에 따른 막대그래프 그리기


```python
# 데이터 프레임으로 성별 생존 확률 확인하기
df[['Survived', 'Sex']].groupby(['Sex']).mean()
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Survived</th>
    </tr>
    <tr>
      <th>Sex</th>
      <th></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>female</th>
      <td>0.742038</td>
    </tr>
    <tr>
      <th>male</th>
      <td>0.188908</td>
    </tr>
  </tbody>
</table>
</div>




```python
# Cross tab으로 성별 생존여부 확인하기
pd.crosstab(df['Survived'], df['Sex']).style.background_gradient('summer_r')
```




<style type="text/css">
#T_11813_row0_col0, #T_11813_row1_col1 {
  background-color: #ffff66;
  color: #000000;
}
#T_11813_row0_col1, #T_11813_row1_col0 {
  background-color: #008066;
  color: #f1f1f1;
}
</style>
<table id="T_11813">
  <thead>
    <tr>
      <th class="index_name level0" >Sex</th>
      <th id="T_11813_level0_col0" class="col_heading level0 col0" >female</th>
      <th id="T_11813_level0_col1" class="col_heading level0 col1" >male</th>
    </tr>
    <tr>
      <th class="index_name level0" >Survived</th>
      <th class="blank col0" >&nbsp;</th>
      <th class="blank col1" >&nbsp;</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th id="T_11813_level0_row0" class="row_heading level0 row0" >0</th>
      <td id="T_11813_row0_col0" class="data row0 col0" >81</td>
      <td id="T_11813_row0_col1" class="data row0 col1" >468</td>
    </tr>
    <tr>
      <th id="T_11813_level0_row1" class="row_heading level0 row1" >1</th>
      <td id="T_11813_row1_col0" class="data row1 col0" >233</td>
      <td id="T_11813_row1_col1" class="data row1 col1" >109</td>
    </tr>
  </tbody>
</table>





```python
#막대그래프로 성별 생존여부 확인하기
f, ax = plt.subplots(1, 2, figsize=(18, 8))
df[['Sex', 'Survived']].groupby(['Sex']).mean().plot.bar(ax=ax[0])
ax[0].set_title('Survived vs Sex')
sns.countplot(x='Sex', hue='Survived', data=df, ax=ax[1])
ax[1].set_title('Sex:Survived vs Dead')
```
![image](https://github.com/liayeoni/study/assets/154586550/a13d98f0-388d-4bbb-b13e-6c955e1f902e)
    


- 여성 생존 확률이 남성 생존 확률보다 월등히 높다.

#### 03-02. Pclass에 따른 생존여부 확인하기(Ordinal Features)


```python
#데이터 프레임으로 등급별 생존여부 확인
df[['Pclass','Survived']].groupby(['Pclass']).sum()
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Survived</th>
    </tr>
    <tr>
      <th>Pclass</th>
      <th></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>1</th>
      <td>136</td>
    </tr>
    <tr>
      <th>2</th>
      <td>87</td>
    </tr>
    <tr>
      <th>3</th>
      <td>119</td>
    </tr>
  </tbody>
</table>
</div>




```python
# 데이터 프레임으로 생존 확률 확인하기
df[['Survived','Pclass']].groupby('Pclass').mean()
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Survived</th>
    </tr>
    <tr>
      <th>Pclass</th>
      <th></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>1</th>
      <td>0.629630</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0.472826</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0.242363</td>
    </tr>
  </tbody>
</table>
</div>




```python
# Crosstab으로 확인
pd.crosstab(df['Pclass'], df['Survived']).style.background_gradient('summer_r')
```




<style type="text/css">
#T_eb530_row0_col0, #T_eb530_row1_col1 {
  background-color: #ffff66;
  color: #000000;
}
#T_eb530_row0_col1, #T_eb530_row2_col0 {
  background-color: #008066;
  color: #f1f1f1;
}
#T_eb530_row1_col0 {
  background-color: #f1f866;
  color: #000000;
}
#T_eb530_row2_col1 {
  background-color: #58ac66;
  color: #f1f1f1;
}
</style>
<table id="T_eb530">
  <thead>
    <tr>
      <th class="index_name level0" >Survived</th>
      <th id="T_eb530_level0_col0" class="col_heading level0 col0" >0</th>
      <th id="T_eb530_level0_col1" class="col_heading level0 col1" >1</th>
    </tr>
    <tr>
      <th class="index_name level0" >Pclass</th>
      <th class="blank col0" >&nbsp;</th>
      <th class="blank col1" >&nbsp;</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th id="T_eb530_level0_row0" class="row_heading level0 row0" >1</th>
      <td id="T_eb530_row0_col0" class="data row0 col0" >80</td>
      <td id="T_eb530_row0_col1" class="data row0 col1" >136</td>
    </tr>
    <tr>
      <th id="T_eb530_level0_row1" class="row_heading level0 row1" >2</th>
      <td id="T_eb530_row1_col0" class="data row1 col0" >97</td>
      <td id="T_eb530_row1_col1" class="data row1 col1" >87</td>
    </tr>
    <tr>
      <th id="T_eb530_level0_row2" class="row_heading level0 row2" >3</th>
      <td id="T_eb530_row2_col0" class="data row2 col0" >372</td>
      <td id="T_eb530_row2_col1" class="data row2 col1" >119</td>
    </tr>
  </tbody>
</table>





```python
#막대그래프로 확인
f, ax = plt.subplots(1, 2, figsize=(18, 8))
df[['Pclass','Survived']].groupby(['Pclass']).mean().sort_values(by='Survived', ascending=False).plot.bar(ax=ax[0])
ax[0].set_title('Survived vs Pclass')
sns.countplot(x='Pclass', hue='Survived', data=df, ax=ax[1])
ax[1].set_title('Pclass : Survived vs Dead')
```
![image](https://github.com/liayeoni/study/assets/154586550/b547cda5-cd66-4e82-8a92-f47b921023a2)
    


- 등급이 높을 수록 생존 확률이 높다.
- 3등급 탑승객 수가 제일 많지만 생존 확률이 가장 낮다.


```python
# catplot으로 등급별, 성별 생존 여부 확인
sns.catplot(x='Pclass', y='Survived', hue='Sex', data=df, kind='point')
```
![image](https://github.com/liayeoni/study/assets/154586550/abf58e5d-afa9-4a77-98fa-d8698f385460)
    


- 성별 관계없이 1등급 탑승객의 생존 여부가 높고, 남성보다 여성의 생존 확률이 높다.

#### 03-03. Age에 따른 생존여부 확인하기(Continuous Features)


```python
# 최연장, 최연소 탑승객 나이, 평균 나이 확인
print('The Oldest Passenger\'s age : {} Years'.format(df['Age'].max()))
print('The Youngest Passenger\'s age : {} Years'.format(df['Age'].min()))
print('The Average Age of All Passengers is : {:.1f} Years'.format(df['Age'].mean()))
```

    The Oldest Passenger's age : 80.0 Years
    The Youngest Passenger's age : 0.42 Years
    The Average Age of All Passengers is : 29.7 Years
    


```python
# kdeplot으로 나이 별 생존 여부 분포 확인
f, ax = plt.subplots(1, 1, figsize=(9, 5))
sns.kdeplot(df[df['Survived']==0]['Age'], ax=ax)
sns.kdeplot(df[df['Survived']==1]['Age'], ax=ax)
plt.legend(['Survived = 0', 'Survived = 1'])
plt.show()
```


    
![image](https://github.com/liayeoni/study/assets/154586550/f385d62a-34b6-43fa-b630-29ba0f85e178)
    


- 나이가 어린 탑승객의 생존 확률이 높다.


```python
# Pclass 별 나이 분포 확인
sns.kdeplot(df[df['Pclass']==1]['Age'])
sns.kdeplot(df[df['Pclass']==2]['Age'])
sns.kdeplot(df[df['Pclass']==3]['Age'])
plt.title('Age Distribution within classes')
plt.legend(['1st Class', '2nd Class', '3rd Class'])
```
![image](https://github.com/liayeoni/study/assets/154586550/8219de17-7ec8-4b17-9e86-8a4f2558fafd)
    


- 2등급, 3등급에 어린이의 분포가 많음.
- 1등급에 나이 많은 탑승객의 비중이 크다.


```python
# 탑승 등급별, 나이별 생존 분포 확인 
f, ax = plt.subplots(1, 3, figsize=(18, 8))
sns.kdeplot(df[(df['Survived']==1) & (df['Pclass']==1)]['Age'], ax=ax[0])
sns.kdeplot(df[(df['Survived']==0) & (df['Pclass']==1)]['Age'], ax=ax[0])
ax[0].set_title('1st Class')

sns.kdeplot(df[(df['Survived']==1) & (df['Pclass']==2)]['Age'], ax=ax[1])
sns.kdeplot(df[(df['Survived']==0) & (df['Pclass']==2)]['Age'], ax=ax[1])
ax[1].set_title('2st Class')

sns.kdeplot(df[(df['Survived']==1) & (df['Pclass']==3)]['Age'], ax=ax[2])
sns.kdeplot(df[(df['Survived']==0) & (df['Pclass']==3)]['Age'], ax=ax[2])
ax[2].set_title('3st Class')
plt.legend(['Survived = 1', 'Survived = 0'])
```
![image](https://github.com/liayeoni/study/assets/154586550/1d81aae3-fc88-4573-b81c-ab70454e9f60)
    


- 등급에 상관없이 어린 아이들의 생존 확률은 높다.
- 1등급 탑승객 중 20-40대 탑승객의 생존 확률이 높다.


```python
# 성별, 나이별 생존 확률 분포
f, ax = plt.subplots(1, 2, figsize=(18, 8))
sns.violinplot(x='Pclass', y='Age', hue='Survived', data=df, split=True, ax=ax[0])
sns.violinplot(x='Sex', y='Age', hue='Survived', data=df, split=True, ax=ax[1])
```
![image](https://github.com/liayeoni/study/assets/154586550/ebfb8389-1a51-4d5b-9516-e957656e2326)
    


- 아동의 수는 등급이 낮아질 수록 많다.
- 10세 이하의 탑승객의 생존비율이 높다.
- 20-50대의 1등급 탑승객의 생존비율이 높다.

#### 03-04. Embarked 에 따른 생존 확률 확인하기


```python
#탑승구별 생존 여부 확인하기
df[['Embarked','Survived']].groupby('Embarked').value_counts()
```




    Embarked  Survived
    C         1            93
              0            75
    Q         0            47
              1            30
    S         0           427
              1           217
    Name: count, dtype: int64




```python
f, ax = plt.subplots(2, 2, figsize=(17, 13))

#countplot으로 탑승구별 생존여부 확인하기
sns.countplot(data=df, x='Embarked', hue='Survived', ax=ax[0, 0])
ax[0,0].set_title('Embarked vs Survived')

# 탑승구별 평균 생존 확률 확인
df[['Embarked', 'Survived']].groupby('Embarked').mean().plot.bar(ax=ax[0,1])
ax[0,1].set_title('Embarked vs Survival Ratio')

# 탑승구별, 성별 탑승객 확인
sns.countplot(data=df, x='Embarked', hue='Sex', ax=ax[1, 0])
ax[1,0].set_title('Embarked vs Sex')

#탑승구별, 등급별 탑승객 확인
sns.countplot(data=df, x='Embarked', hue='Pclass', ax=ax[1, 1])
ax[1, 1].set_title('Embarked vs Pclass')

```
![image](https://github.com/liayeoni/study/assets/154586550/903486ae-fe51-4fa6-a707-304b86f6ca12)
    


- C 탑승구에서 탄 탑승객의 평균 생존확률이 높고, S등급에서 탄 탑승객의 평균 생존확률이 낮다.
- C 탑승구에서 탄 탑승객 중 1등급 탑승객의 비율이 높다.
- S 탑승구에서 탄 탑승객 중 3등급 탑승객의 비율이 높다.


```python
# catplot으로 성별, 등급별, 탑승구 별 생존 여부 확인하기
sns.catplot(data=df, x='Pclass', y='Survived', hue='Sex', col='Embarked', kind='point')
```
![image](https://github.com/liayeoni/study/assets/154586550/8a29684f-410a-4300-9501-6e556e978c08)
    


- 1등급과 2등급의 여성 탑승객의 생존확률은 1에 근접한다.
- S 항구의 3등급 탑승객은 남성과 여성 모두 생존율이 낮다.
- Q 항구의 남성 탑승객 생존율은 매우 낮고, Q항구 탑승객의 대부분은 3등급 탑승객이다.

#### 03-05. Family 수로 생존여부 확인하기

- Family = SibSp + Parch + 1(본인)
- Family로 합쳐서 분석


```python
# FamilySize 컬럼 생성
df['FamilySize'] = df['SibSp'] + df['Parch'] + 1
df_test['FamilySize'] = df['SibSp'] + df['Parch'] + 1
```


```python
# 최대, 최소 가족 규모 구하기
print('Maximum size of Family : ', df['FamilySize'].max())
print('Minimum size of Family : ', df['FamilySize'].min())
```

    Maximum size of Family :  11
    Minimum size of Family :  1
    


```python
df['FamilySize'].head()
```




    0    2
    1    2
    2    1
    3    2
    4    1
    Name: FamilySize, dtype: int64




```python
f, ax = plt.subplots(1, 3, figsize=(25, 8))

# 막대 그래프로 가족 규모 별 생존 확률 확인
df[['FamilySize', 'Survived']].groupby('FamilySize').mean().sort_values(by='Survived', ascending=False).plot.bar(ax=ax[0])
ax[0].set_title('FamilySize vs Survived')

# 막대 그래프로 가족 규모 별 생존 여부 확인
sns.countplot(data=df, x='FamilySize', hue='Survived', ax=ax[1])
ax[1].set_title('FamilySize vs Survived')

# pointplot으로 가족 규모 별 생존 확률 확인
sns.pointplot(data=df, x='FamilySize', y='Survived', ax=ax[2])
ax[2].set_title('FamilySize vs Survived')

```
![image](https://github.com/liayeoni/study/assets/154586550/67e9b464-052a-42e0-bbcc-7edfb854069f)
    


- 2-3명의 가족 규모에서 가장 높은 생존 확률을 보인다.


```python
# crosstab으로 가족 규모 별 탑승 등급 확인
pd.crosstab(df['FamilySize'], df['Pclass']).style.background_gradient('summer_r')
```




<style type="text/css">
#T_f0909_row0_col0, #T_f0909_row0_col1, #T_f0909_row0_col2 {
  background-color: #008066;
  color: #f1f1f1;
}
#T_f0909_row1_col0 {
  background-color: #5bad66;
  color: #f1f1f1;
}
#T_f0909_row1_col1 {
  background-color: #acd666;
  color: #000000;
}
#T_f0909_row1_col2 {
  background-color: #d6eb66;
  color: #000000;
}
#T_f0909_row2_col0 {
  background-color: #c7e366;
  color: #000000;
}
#T_f0909_row2_col1 {
  background-color: #b3d966;
  color: #000000;
}
#T_f0909_row2_col2 {
  background-color: #deee66;
  color: #000000;
}
#T_f0909_row3_col0 {
  background-color: #eff766;
  color: #000000;
}
#T_f0909_row3_col1 {
  background-color: #dfef66;
  color: #000000;
}
#T_f0909_row3_col2, #T_f0909_row4_col1, #T_f0909_row5_col1 {
  background-color: #fdfe66;
  color: #000000;
}
#T_f0909_row4_col0, #T_f0909_row4_col2, #T_f0909_row6_col2 {
  background-color: #fbfd66;
  color: #000000;
}
#T_f0909_row5_col0 {
  background-color: #f6fa66;
  color: #000000;
}
#T_f0909_row5_col2 {
  background-color: #f7fb66;
  color: #000000;
}
#T_f0909_row6_col0, #T_f0909_row6_col1, #T_f0909_row7_col0, #T_f0909_row7_col1, #T_f0909_row7_col2, #T_f0909_row8_col0, #T_f0909_row8_col1, #T_f0909_row8_col2 {
  background-color: #ffff66;
  color: #000000;
}
</style>
<table id="T_f0909">
  <thead>
    <tr>
      <th class="index_name level0" >Pclass</th>
      <th id="T_f0909_level0_col0" class="col_heading level0 col0" >1</th>
      <th id="T_f0909_level0_col1" class="col_heading level0 col1" >2</th>
      <th id="T_f0909_level0_col2" class="col_heading level0 col2" >3</th>
    </tr>
    <tr>
      <th class="index_name level0" >FamilySize</th>
      <th class="blank col0" >&nbsp;</th>
      <th class="blank col1" >&nbsp;</th>
      <th class="blank col2" >&nbsp;</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th id="T_f0909_level0_row0" class="row_heading level0 row0" >1</th>
      <td id="T_f0909_row0_col0" class="data row0 col0" >109</td>
      <td id="T_f0909_row0_col1" class="data row0 col1" >104</td>
      <td id="T_f0909_row0_col2" class="data row0 col2" >324</td>
    </tr>
    <tr>
      <th id="T_f0909_level0_row1" class="row_heading level0 row1" >2</th>
      <td id="T_f0909_row1_col0" class="data row1 col0" >70</td>
      <td id="T_f0909_row1_col1" class="data row1 col1" >34</td>
      <td id="T_f0909_row1_col2" class="data row1 col2" >57</td>
    </tr>
    <tr>
      <th id="T_f0909_level0_row2" class="row_heading level0 row2" >3</th>
      <td id="T_f0909_row2_col0" class="data row2 col0" >24</td>
      <td id="T_f0909_row2_col1" class="data row2 col1" >31</td>
      <td id="T_f0909_row2_col2" class="data row2 col2" >47</td>
    </tr>
    <tr>
      <th id="T_f0909_level0_row3" class="row_heading level0 row3" >4</th>
      <td id="T_f0909_row3_col0" class="data row3 col0" >7</td>
      <td id="T_f0909_row3_col1" class="data row3 col1" >13</td>
      <td id="T_f0909_row3_col2" class="data row3 col2" >9</td>
    </tr>
    <tr>
      <th id="T_f0909_level0_row4" class="row_heading level0 row4" >5</th>
      <td id="T_f0909_row4_col0" class="data row4 col0" >2</td>
      <td id="T_f0909_row4_col1" class="data row4 col1" >1</td>
      <td id="T_f0909_row4_col2" class="data row4 col2" >12</td>
    </tr>
    <tr>
      <th id="T_f0909_level0_row5" class="row_heading level0 row5" >6</th>
      <td id="T_f0909_row5_col0" class="data row5 col0" >4</td>
      <td id="T_f0909_row5_col1" class="data row5 col1" >1</td>
      <td id="T_f0909_row5_col2" class="data row5 col2" >17</td>
    </tr>
    <tr>
      <th id="T_f0909_level0_row6" class="row_heading level0 row6" >7</th>
      <td id="T_f0909_row6_col0" class="data row6 col0" >0</td>
      <td id="T_f0909_row6_col1" class="data row6 col1" >0</td>
      <td id="T_f0909_row6_col2" class="data row6 col2" >12</td>
    </tr>
    <tr>
      <th id="T_f0909_level0_row7" class="row_heading level0 row7" >8</th>
      <td id="T_f0909_row7_col0" class="data row7 col0" >0</td>
      <td id="T_f0909_row7_col1" class="data row7 col1" >0</td>
      <td id="T_f0909_row7_col2" class="data row7 col2" >6</td>
    </tr>
    <tr>
      <th id="T_f0909_level0_row8" class="row_heading level0 row8" >11</th>
      <td id="T_f0909_row8_col0" class="data row8 col0" >0</td>
      <td id="T_f0909_row8_col1" class="data row8 col1" >0</td>
      <td id="T_f0909_row8_col2" class="data row8 col2" >7</td>
    </tr>
  </tbody>
</table>




- 가족 규모가 큰 탑승객(7~11명) 은 3등급 탑승객이다.
- 혼자인 탑승객의 수가 가장 많다.
- 2명 탑승객 중 1등급 탑승객이 많고, 3명 탑승객은 3등급, 4명 탑승객은 2등급 탑승객이 가장 많다.

#### 03-06. Fare별 생존 여부 확인(Continuous Feature)
- sns.distplot : histplot과 displot으로 대체됨. histplot과 density curve 함께 분포확인 가능
- .skew() : 비대칭도. 평균에 대해 최빈값이 얼마나 치우쳐져 있는 나타내는 척도


```python
# Fare 최댓값, 최솟값 확인
print('The Highest Fare was : ', df['Fare'].max())
print('The Lowest Fare was : ', df['Fare'].min())
print('The Average Fare was : ', df['Fare'].mean())
```

    The Highest Fare was :  512.3292
    The Lowest Fare was :  0.0
    The Average Fare was :  32.204207968574636
    


```python
# distplot으로 Fare  컬럼의 skewness 확인
sns.distplot(df['Fare'], color ='b')
```
![image](https://github.com/liayeoni/study/assets/154586550/0eb181d3-eb06-4ee5-a119-13b8186dfb6b)
    



```python
# 등급별 Fare 분포 확인
f, ax = plt.subplots(1, 3, figsize=(20, 8))
sns.histplot(x=df[df['Pclass']==1]['Fare'], data=df, ax=ax[0], kde=True)
ax[0].set_title('Fares in Pclass 1')

sns.histplot(x=df[df['Pclass']==2]['Fare'], data=df, ax=ax[1], kde=True)
ax[1].set_title('Fares in Pclass 2')

sns.histplot(x=df[df['Pclass']==3]['Fare'], data=df, ax=ax[2], kde=True)
ax[2].set_title('Fares in Pclass 3')
```
![image](https://github.com/liayeoni/study/assets/154586550/d7c5a499-2d68-465c-a6fb-f3d4680ee9d3)
    


- 1등급에서 가격의 차이가 큰 것을 확인할 수 있다.

#### Inital값 채우기


```python
df['Initial'] = 0
for i in df:
    df['Initial'] = df.Name.str.extract('([A-Za-z]+)\.')
```


```python
pd.crosstab(df.Initial, df.Sex).T.style.background_gradient(cmap='summer_r')
```




<style type="text/css">
#T_24417_row0_col0, #T_24417_row0_col1, #T_24417_row0_col3, #T_24417_row0_col4, #T_24417_row0_col5, #T_24417_row0_col7, #T_24417_row0_col8, #T_24417_row0_col12, #T_24417_row0_col15, #T_24417_row0_col16, #T_24417_row1_col2, #T_24417_row1_col6, #T_24417_row1_col9, #T_24417_row1_col10, #T_24417_row1_col11, #T_24417_row1_col13, #T_24417_row1_col14 {
  background-color: #ffff66;
  color: #000000;
}
#T_24417_row0_col2, #T_24417_row0_col6, #T_24417_row0_col9, #T_24417_row0_col10, #T_24417_row0_col11, #T_24417_row0_col13, #T_24417_row0_col14, #T_24417_row1_col0, #T_24417_row1_col1, #T_24417_row1_col3, #T_24417_row1_col4, #T_24417_row1_col5, #T_24417_row1_col7, #T_24417_row1_col8, #T_24417_row1_col12, #T_24417_row1_col15, #T_24417_row1_col16 {
  background-color: #008066;
  color: #f1f1f1;
}
</style>
<table id="T_24417">
  <thead>
    <tr>
      <th class="index_name level0" >Initial</th>
      <th id="T_24417_level0_col0" class="col_heading level0 col0" >Capt</th>
      <th id="T_24417_level0_col1" class="col_heading level0 col1" >Col</th>
      <th id="T_24417_level0_col2" class="col_heading level0 col2" >Countess</th>
      <th id="T_24417_level0_col3" class="col_heading level0 col3" >Don</th>
      <th id="T_24417_level0_col4" class="col_heading level0 col4" >Dr</th>
      <th id="T_24417_level0_col5" class="col_heading level0 col5" >Jonkheer</th>
      <th id="T_24417_level0_col6" class="col_heading level0 col6" >Lady</th>
      <th id="T_24417_level0_col7" class="col_heading level0 col7" >Major</th>
      <th id="T_24417_level0_col8" class="col_heading level0 col8" >Master</th>
      <th id="T_24417_level0_col9" class="col_heading level0 col9" >Miss</th>
      <th id="T_24417_level0_col10" class="col_heading level0 col10" >Mlle</th>
      <th id="T_24417_level0_col11" class="col_heading level0 col11" >Mme</th>
      <th id="T_24417_level0_col12" class="col_heading level0 col12" >Mr</th>
      <th id="T_24417_level0_col13" class="col_heading level0 col13" >Mrs</th>
      <th id="T_24417_level0_col14" class="col_heading level0 col14" >Ms</th>
      <th id="T_24417_level0_col15" class="col_heading level0 col15" >Rev</th>
      <th id="T_24417_level0_col16" class="col_heading level0 col16" >Sir</th>
    </tr>
    <tr>
      <th class="index_name level0" >Sex</th>
      <th class="blank col0" >&nbsp;</th>
      <th class="blank col1" >&nbsp;</th>
      <th class="blank col2" >&nbsp;</th>
      <th class="blank col3" >&nbsp;</th>
      <th class="blank col4" >&nbsp;</th>
      <th class="blank col5" >&nbsp;</th>
      <th class="blank col6" >&nbsp;</th>
      <th class="blank col7" >&nbsp;</th>
      <th class="blank col8" >&nbsp;</th>
      <th class="blank col9" >&nbsp;</th>
      <th class="blank col10" >&nbsp;</th>
      <th class="blank col11" >&nbsp;</th>
      <th class="blank col12" >&nbsp;</th>
      <th class="blank col13" >&nbsp;</th>
      <th class="blank col14" >&nbsp;</th>
      <th class="blank col15" >&nbsp;</th>
      <th class="blank col16" >&nbsp;</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th id="T_24417_level0_row0" class="row_heading level0 row0" >female</th>
      <td id="T_24417_row0_col0" class="data row0 col0" >0</td>
      <td id="T_24417_row0_col1" class="data row0 col1" >0</td>
      <td id="T_24417_row0_col2" class="data row0 col2" >1</td>
      <td id="T_24417_row0_col3" class="data row0 col3" >0</td>
      <td id="T_24417_row0_col4" class="data row0 col4" >1</td>
      <td id="T_24417_row0_col5" class="data row0 col5" >0</td>
      <td id="T_24417_row0_col6" class="data row0 col6" >1</td>
      <td id="T_24417_row0_col7" class="data row0 col7" >0</td>
      <td id="T_24417_row0_col8" class="data row0 col8" >0</td>
      <td id="T_24417_row0_col9" class="data row0 col9" >182</td>
      <td id="T_24417_row0_col10" class="data row0 col10" >2</td>
      <td id="T_24417_row0_col11" class="data row0 col11" >1</td>
      <td id="T_24417_row0_col12" class="data row0 col12" >0</td>
      <td id="T_24417_row0_col13" class="data row0 col13" >125</td>
      <td id="T_24417_row0_col14" class="data row0 col14" >1</td>
      <td id="T_24417_row0_col15" class="data row0 col15" >0</td>
      <td id="T_24417_row0_col16" class="data row0 col16" >0</td>
    </tr>
    <tr>
      <th id="T_24417_level0_row1" class="row_heading level0 row1" >male</th>
      <td id="T_24417_row1_col0" class="data row1 col0" >1</td>
      <td id="T_24417_row1_col1" class="data row1 col1" >2</td>
      <td id="T_24417_row1_col2" class="data row1 col2" >0</td>
      <td id="T_24417_row1_col3" class="data row1 col3" >1</td>
      <td id="T_24417_row1_col4" class="data row1 col4" >6</td>
      <td id="T_24417_row1_col5" class="data row1 col5" >1</td>
      <td id="T_24417_row1_col6" class="data row1 col6" >0</td>
      <td id="T_24417_row1_col7" class="data row1 col7" >2</td>
      <td id="T_24417_row1_col8" class="data row1 col8" >40</td>
      <td id="T_24417_row1_col9" class="data row1 col9" >0</td>
      <td id="T_24417_row1_col10" class="data row1 col10" >0</td>
      <td id="T_24417_row1_col11" class="data row1 col11" >0</td>
      <td id="T_24417_row1_col12" class="data row1 col12" >517</td>
      <td id="T_24417_row1_col13" class="data row1 col13" >0</td>
      <td id="T_24417_row1_col14" class="data row1 col14" >0</td>
      <td id="T_24417_row1_col15" class="data row1 col15" >6</td>
      <td id="T_24417_row1_col16" class="data row1 col16" >1</td>
    </tr>
  </tbody>
</table>





```python
df['Initial'].replace(['Mlle','Mme','Ms','Dr','Major','Lady','Countess','Jonkheer','Col','Rev','Capt','Sir','Don'],['Miss','Miss','Miss','Mr','Mr','Mrs','Mrs','Other','Other','Other','Mr','Mr','Mr'], inplace=True)
```


```python
df.groupby('Initial')['Age'].mean()
```




    Initial
    Master     4.574167
    Miss      21.860000
    Mr        32.739609
    Mrs       35.981818
    Other     45.888889
    Name: Age, dtype: float64



### 04. Nan값 처리

### Filling Nan Age


```python
# 평균값으로 나이 채우기
df.loc[(df.Age.isnull())&(df.Initial=='Mr'), 'Age'] = 33
df.loc[(df.Age.isnull())&(df.Initial=='Mrs'), 'Age'] = 36
df.loc[(df.Age.isnull())&(df.Initial=='Master'), 'Age'] = 5
df.loc[(df.Age.isnull())&(df.Initial=='Miss'), 'Age'] = 22
df.loc[(df.Age.isnull())&(df.Initial=='Other'), 'Age'] = 46
```


```python
df.Age.isnull().any()
```




    False




```python
f, ax = plt.subplots(1, 2, figsize=(20, 10))
df[df['Survived']==0].Age.plot.hist(ax=ax[0], bins=20, edgecolor='black', color='red') # Survived=0 인 사람의 나이 분포
ax[0].set_title('Survived = 0')
x1 = list(range(0, 85, 5)) # x축 범위 지정
ax[0].set_xticks(x1)
df[df['Survived']==1].Age.plot.hist(ax=ax[1], color='green', bins=20, edgecolor='black')  # Survived=1 인 사람의 나이 분포
ax[1].set_title('Survived = 1')
x2 = list(range(0, 85, 5))
ax[1].set_xticks(x2)
plt.show()
```


    
![image](https://github.com/liayeoni/study/assets/154586550/5ee52a90-bc1c-4b28-ac7a-00c7893ae0d3)
    


1 ) 5세 이하의 아이들은 많은 수로 생존하였다.\
2) 가장 나이가 많은 사람은 생존하였다.\
3) 30-40 나이대에서 사망률이 가장 높다.

### Embarked 채우기


```python
df['Embarked'].fillna('S', inplace=True)
```


```python
heatmap_data = df.select_dtypes(include='number') # value가 숫자인 컬럼만 불러오
sns.heatmap(heatmap_data.corr(), annot=True, cmap='RdYlGn', linewidths=0.2)
fig = plt.gcf()
fig.set_size_inches(10,8)
plt.show()
```
![image](https://github.com/liayeoni/study/assets/154586550/969aee44-61f4-474e-a6a5-8ef274b0341a)
    


## Feature Engineering

### Age 나이대 구분하기


```python
df['Age_band'] = 0
df.loc[df['Age'] <= 16, 'Age_band'] = 0
df.loc[(df['Age'] > 16) & (df['Age'] <= 32), 'Age_band'] = 1
df.loc[(df['Age'] > 32) & (df['Age'] <= 48), 'Age_band'] = 2
df.loc[(df['Age'] > 48) & (df['Age'] <= 64), 'Age_band'] = 3
df.loc[df['Age'] > 64, 'Age_band'] = 4
```


```python
df['Age_band'].value_counts().to_frame().style.background_gradient(cmap = 'summer')
```




<style type="text/css">
#T_32aba_row0_col0 {
  background-color: #ffff66;
  color: #000000;
}
#T_32aba_row1_col0 {
  background-color: #d8ec66;
  color: #000000;
}
#T_32aba_row2_col0 {
  background-color: #40a066;
  color: #f1f1f1;
}
#T_32aba_row3_col0 {
  background-color: #289366;
  color: #f1f1f1;
}
#T_32aba_row4_col0 {
  background-color: #008066;
  color: #f1f1f1;
}
</style>
<table id="T_32aba">
  <thead>
    <tr>
      <th class="blank level0" >&nbsp;</th>
      <th id="T_32aba_level0_col0" class="col_heading level0 col0" >count</th>
    </tr>
    <tr>
      <th class="index_name level0" >Age_band</th>
      <th class="blank col0" >&nbsp;</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th id="T_32aba_level0_row0" class="row_heading level0 row0" >1</th>
      <td id="T_32aba_row0_col0" class="data row0 col0" >382</td>
    </tr>
    <tr>
      <th id="T_32aba_level0_row1" class="row_heading level0 row1" >2</th>
      <td id="T_32aba_row1_col0" class="data row1 col0" >325</td>
    </tr>
    <tr>
      <th id="T_32aba_level0_row2" class="row_heading level0 row2" >0</th>
      <td id="T_32aba_row2_col0" class="data row2 col0" >104</td>
    </tr>
    <tr>
      <th id="T_32aba_level0_row3" class="row_heading level0 row3" >3</th>
      <td id="T_32aba_row3_col0" class="data row3 col0" >69</td>
    </tr>
    <tr>
      <th id="T_32aba_level0_row4" class="row_heading level0 row4" >4</th>
      <td id="T_32aba_row4_col0" class="data row4 col0" >11</td>
    </tr>
  </tbody>
</table>




### Fare 구분하기


```python
df['Fare_Range'] = pd.qcut(df['Fare'], 4)
df.groupby(['Fare_Range'])['Survived'].mean().to_frame().style.background_gradient(cmap='summer_r')
```




<style type="text/css">
#T_951aa_row0_col0 {
  background-color: #ffff66;
  color: #000000;
}
#T_951aa_row1_col0 {
  background-color: #b9dc66;
  color: #000000;
}
#T_951aa_row2_col0 {
  background-color: #54aa66;
  color: #f1f1f1;
}
#T_951aa_row3_col0 {
  background-color: #008066;
  color: #f1f1f1;
}
</style>
<table id="T_951aa">
  <thead>
    <tr>
      <th class="blank level0" >&nbsp;</th>
      <th id="T_951aa_level0_col0" class="col_heading level0 col0" >Survived</th>
    </tr>
    <tr>
      <th class="index_name level0" >Fare_Range</th>
      <th class="blank col0" >&nbsp;</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th id="T_951aa_level0_row0" class="row_heading level0 row0" >(-0.001, 7.91]</th>
      <td id="T_951aa_row0_col0" class="data row0 col0" >0.197309</td>
    </tr>
    <tr>
      <th id="T_951aa_level0_row1" class="row_heading level0 row1" >(7.91, 14.454]</th>
      <td id="T_951aa_row1_col0" class="data row1 col0" >0.303571</td>
    </tr>
    <tr>
      <th id="T_951aa_level0_row2" class="row_heading level0 row2" >(14.454, 31.0]</th>
      <td id="T_951aa_row2_col0" class="data row2 col0" >0.454955</td>
    </tr>
    <tr>
      <th id="T_951aa_level0_row3" class="row_heading level0 row3" >(31.0, 512.329]</th>
      <td id="T_951aa_row3_col0" class="data row3 col0" >0.581081</td>
    </tr>
  </tbody>
</table>




- Fare값이 증가할 수록 생존률도 증가한다.


```python
df['Fare_cat'] = 0
df.loc[df['Fare'] <= 7.91, 'Fare_cat'] = 0
df.loc[(df['Fare'] > 7.91) & (df['Fare'] <= 14.454), 'Fare_cat'] = 1
df.loc[(df['Fare'] > 14.454) & (df['Fare'] <= 31), 'Fare_cat'] = 2
df.loc[(df['Fare'] > 31) & (df['Fare'] <= 513), 'Fare_cat'] = 3
```


```python
sns.catplot(x='Fare_cat', y='Survived', data=df, hue='Sex', kind = 'point')
plt.show()
```

![image](https://github.com/liayeoni/study/assets/154586550/9fafb63d-1c9c-47f8-ace2-0754a5fb169b)
    


### 문자값을 숫자값으로 바꾸기


```python
# 성별, 탑승구, 이니셜
df['Sex'].replace(['male','female'],[0, 1], inplace=True)
df['Embarked'].replace(['S','C','Q'], [0, 1, 2], inplace=True)
df['Initial'].replace(['Mr','Mrs','Miss','Master','Other'], [0, 1, 2, 3, 4], inplace=True)
```

### 필요없는 컬럼 삭제
- 이름, 나이, 티켓, 요금, Cabin, Fare_Range, PassengerId


```python
df.drop(['Name','Age','Ticket','Fare','Cabin','Fare_Range','PassengerId'], axis=1, inplace=True)
sns.heatmap(df.corr(), annot=True, cmap='RdYlGn', linewidths=0.2, annot_kws={'size':20})
fig = plt.gcf()
fig.set_size_inches(18, 15)
plt.xticks(fontsize=14)
plt.yticks(fontsize=14)
plt.show()
```
![image](https://github.com/liayeoni/study/assets/154586550/087d44b1-86c9-4484-a748-c7708c0deef5)
    


### Predictive Modeling
1) Logistic Regression
2) Support Vector machines(Linear and radial)
3) Random Forest
4) K-Nearest Neighbours
5) Naive Bayes
6) Decision Tree
7) Logistic Regression


```python
from sklearn.linear_model import LogisticRegression # logistic Regression
from sklearn import svm # support vector Machine
from sklearn.ensemble import RandomForestClassifier # Random Forest
from sklearn.neighbors import KNeighborsClassifier # KNN
from sklearn.naive_bayes import GaussianNB #Naive bayes
from sklearn.tree import DecisionTreeClassifier # Decision Tree
from sklearn.model_selection import train_test_split # training and test data split
from sklearn import metrics
from sklearn.metrics import confusion_matrix # for confusion matrix
```


```python
train, test = train_test_split(df, test_size = 0.3, random_state = 0, stratify = df['Survived'])
train_X = train[train.columns[1:]]
train_Y = train[train.columns[:1]]
test_X = test[test.columns[1:]]
test_Y = test[test.columns[:1]]
X=df[df.columns[1:]]
Y=df['Survived']
```

### Radial Support Vector Machines(rbf-SVM)


```python
model = svm.SVC(kernel='rbf', C=1, gamma =0.1)
model.fit(train_X, train_Y)
prediction1 = model.predict(test_X)
print('Accuracy for rbf SVM is : ', metrics.accuracy_score(prediction1, test_Y))
```

    Accuracy for rbf SVM is :  0.835820895522388
    

### Linear Support Vector machine(linear-SVM)


```python
model = svm.SVC(kernel='linear', C=0.1, gamma=0.1)
model.fit(train_X, train_Y)
prediction2 = model.predict(test_X)
print('Accuracy for linear SVM is', metrics.accuracy_score(prediction2, test_Y))
```

    Accuracy for linear SVM is 0.8059701492537313
    

### Logistic Regression


```python
model = LogisticRegression()
model.fit(train_X, train_Y)
prediction3 = model.predict(test_X)
print('The accuracy of the Logistic Regression is ', metrics.accuracy_score(prediction3, test_Y))
```

    The accuracy of the Logistic Regression is  0.8171641791044776
    

### Decision Tree


```python
model = DecisionTreeClassifier()
model.fit(train_X, train_Y)
prediction4 = model.predict(test_X)
print('The accuracy of the Decision Tree is', metrics.accuracy_score(prediction4, test_Y))
```

    The accuracy of the Decision Tree is 0.8022388059701493
    

### K-Nearest Neighbours(KNN)


```python
model = KNeighborsClassifier()
model.fit(train_X, train_Y)
prediction5 = model.predict(test_X)
print('The accuracy of the KNN is', metrics.accuracy_score(prediction5, test_Y))
```

    The accuracy of the KNN is 0.8097014925373134
    


```python
a_index = list(range(1, 11))
a = pd.Series()
x = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]
for i in list(range(1, 11)):
    model = KNeighborsClassifier(n_neighbors=i)
    model.fit(train_X, train_Y)
    prediction = model.predict(test_X)
    a = pd.concat([a, pd.Series(metrics.accuracy_score(prediction, test_Y))], ignore_index=True)
plt.plot(a_index, a)
plt.xticks(x)
fig = plt.gcf()
fig.set_size_inches(12, 6)
plt.show()
print('Accuracies for different values of n are : ', a.values, 'with the max value as', a.values.max())
```
![image](https://github.com/liayeoni/study/assets/154586550/27a22c4e-6568-49a3-8e6d-7aacae3a1b83)
    


    Accuracies for different values of n are :  [0.75746269 0.76119403 0.80223881 0.79477612 0.80970149 0.80223881
     0.82089552 0.82462687 0.8358209  0.82835821] with the max value as 0.835820895522388
    

### Gaussian Naive Bayes


```python
model = GaussianNB()
model.fit(train_X, train_Y)
prediction6 = model.predict(test_X)
print('The accuracy of the NaiveBayes is', metrics.accuracy_score(prediction6, test_Y))
```

    The accuracy of the NaiveBayes is 0.8097014925373134
    

### RandomForests


```python
model = RandomForestClassifier()
model.fit(train_X, train_Y)
prediction7 = model.predict(test_X)
print('The accuracy of the Random Forests is', metrics.accuracy_score(prediction7, test_Y))
```

    The accuracy of the Random Forests is 0.8171641791044776
    

### Cross Validation
- 모델 변동성 : 훈련 및 테스트 데이터가 변하면 정확도도 변한다. 즉, 데이터 셋에따라 정확도가 증가하거나 감속할 수 있다.
- 교차 검증 : 모델 변동성을 극복하고 일반화된 모델을 얻기 위함. 훈련 및 테스트 데이터를 여러 번 반복적으로 나눈어 모델을 평가하는 기술. 교차 검증을 통해 모델이 다양한 데이터에 대해 얼마나 일반화되는지를 더 신뢰할 수 있게 평가할 수 있다.

1. K-Fold Cross Validation : 먼저 데이터셋을 k개의 하위 집합으로 나눔
2. 데이터셋을 (k=5)부분으로 나눈다면 1부분은 테스트를 위해 보류하고 4부분에서 알고리즘을 훈련
3. 다음 반복에서는 테스트 부분을 변경하고 다른 부분에서 알고리즘 훈련 진행. 정확도와 오류를 평균화하여 알고리즘의 평균 정확도를 얻을 수 있다.

- 알고리즘이 특정 훈련 데이터에 대해 과소적합 또는 다른 훈련 세트에 대해 데이터를 과적합할 수 있다. 교차 검증을 사용하면 일반화된 모델을 얻을 수 있다.


```python
from sklearn.model_selection import KFold # for K-Fold cross validation
from sklearn.model_selection import cross_val_score # score evaluation
from sklearn.model_selection import cross_val_predict # prediction
kfold = KFold(n_splits=10, random_state= 22, shuffle=True)

xyz=[]
accuracy = []
std = []
classifiers = ['Linear Svm','Radial Svm','Logistic Regression', 'KNN', 'Decision Tree', 'Naive Bayes', 'Random Forest']
models = [svm.SVC(kernel = 'linear'), svm.SVC(kernel = 'rbf'), LogisticRegression(), KNeighborsClassifier(n_neighbors=9), DecisionTreeClassifier(), GaussianNB(), RandomForestClassifier(n_estimators=100)]

for i in models:
    model = i
    cv_result = cross_val_score(model, X, Y, cv = kfold, scoring = 'accuracy')
    xyz.append(cv_result.mean())
    std.append(cv_result.std())
    accuracy.append(cv_result)
new_models_dataframe2 = pd.DataFrame({'CV Mean':xyz, 'Std':std}, index=classifiers)
new_models_dataframe2
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>CV Mean</th>
      <th>Std</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>Linear Svm</th>
      <td>0.782360</td>
      <td>0.058806</td>
    </tr>
    <tr>
      <th>Radial Svm</th>
      <td>0.828377</td>
      <td>0.057096</td>
    </tr>
    <tr>
      <th>Logistic Regression</th>
      <td>0.801411</td>
      <td>0.036139</td>
    </tr>
    <tr>
      <th>KNN</th>
      <td>0.809263</td>
      <td>0.042580</td>
    </tr>
    <tr>
      <th>Decision Tree</th>
      <td>0.800237</td>
      <td>0.046209</td>
    </tr>
    <tr>
      <th>Naive Bayes</th>
      <td>0.798102</td>
      <td>0.052204</td>
    </tr>
    <tr>
      <th>Random Forest</th>
      <td>0.813745</td>
      <td>0.043283</td>
    </tr>
  </tbody>
</table>
</div>




```python
plt.subplots(figsize=(12, 6))
box = pd.DataFrame(accuracy, index=[classifiers])
box.T.boxplot()
```

![image](https://github.com/liayeoni/study/assets/154586550/ae54c3bb-520e-4367-944c-5583463e6b67)
    



```python
new_models_dataframe2['CV Mean'].plot.barh(width=0.8)
plt.title('Average CV Mean Accuracy')
fig = plt.gcf()
fig.set_size_inches(8, 5)
plt.show()
```
![image](https://github.com/liayeoni/study/assets/154586550/2c64f0e9-c7a9-4875-9b2b-9b4808606c54)
    


### Confusion Matrix


```python
f, ax = plt.subplots(3, 3, figsize=(12, 10))
# rbf-SVM
y_pred = cross_val_predict(svm.SVC(kernel='rbf'), X, Y, cv=10)
sns.heatmap(confusion_matrix(Y, y_pred), ax=ax[0, 0], annot=True, fmt='2.0f')
ax[0, 0].set_title('Matrix for rbf-SVM')

# Linear-SVM
y_pred = cross_val_predict(svm.SVC(kernel='linear'), X, Y, cv=10)
sns.heatmap(confusion_matrix(Y, y_pred), ax=ax[0, 1], annot=True, fmt='2.0f')
ax[0, 1].set_title('Matrix for Linear-SVM')

# KNN
y_pred = cross_val_predict(KNeighborsClassifier(n_neighbors=9), X, Y, cv = 10)
sns.heatmap(confusion_matrix(Y, y_pred), ax=ax[0, 2], annot=True, fmt='2.0f')
ax[0, 2].set_title('Matrix for KNN')

# Random-Forests
y_pred = cross_val_predict(RandomForestClassifier(n_estimators=100), X, Y, cv=10)
sns.heatmap(confusion_matrix(Y, y_pred), ax=ax[1, 0], annot=True, fmt='2.0f')
ax[1, 0].set_title('matrix for Random-Forests')

# Logistic Regression
y_pred = cross_val_predict(LogisticRegression(), X, Y, cv=10)
sns.heatmap(confusion_matrix(Y, y_pred), ax=ax[1, 1], annot=True, fmt='2.0f')
ax[1, 1].set_title('matrix for Logistic Regression')

# Decision Tree
y_pred = cross_val_predict(DecisionTreeClassifier(), X, Y, cv=10)
sns.heatmap(confusion_matrix(Y, y_pred), ax=ax[1, 2], annot=True, fmt='2.0f')
ax[1, 2].set_title('matrix for Decision Tree')

# Naive Bayes
y_pred = cross_val_predict(GaussianNB(), X, Y, cv=10)
sns.heatmap(confusion_matrix(Y, y_pred), ax=ax[2, 0], annot=True, fmt='2.0f')
ax[2, 0].set_title('matrix for Naive Bayes')

plt.subplots_adjust(hspace=0.2, wspace=0.2)
plt.show()
```

![image](https://github.com/liayeoni/study/assets/154586550/15ef01a4-c4d6-494c-bdb9-c5aac5617244)
    


rbf-SVM : 491(for dead) + 247(for survived) / 891 = 82.8%
- 생존하지 못한 경우를 생존했다고 잘못 예측한 경우가 더 많다.
- rbf-SVM 생존하지 못한 경우를 가장 정확하게 예측/ Naive Bayes 생존한 경우를 가장 정확하게 예측 

### Hyper-Parameters Tuning

- Parameter values를 조정하거나 변경하여 더 나은 모델을 얻을 수 있음. SVM 및 RandomForests와 같은 분류기에 대하여 파라미터 튜닝 진행

### SVM
- GridSearchCV() : 그리드 서치 방법을 사용하여 모델의 최적 parameter value를 차즌ㄴ데 사용. 가능한 모든 조합을 사용하여 최적의 조합을 찾음.


```python
from sklearn.model_selection import GridSearchCV
C = [0.05, 0.1, 0.2, 0.3, 0.25, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1]
gamma = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]
kernel = ['rbf', 'linear']
hyper = {'kernel':kernel, 'C':C, 'gamma':gamma}
gd = GridSearchCV(estimator=svm.SVC(), param_grid = hyper, verbose=True)
gd.fit(X, Y)
print(gd.best_score_)
print(gd.best_estimator_)
```

    Fitting 5 folds for each of 240 candidates, totalling 1200 fits
    0.8282593685267716
    SVC(C=0.6, gamma=0.1)
    

### Random Forest


```python
n_estimators = range(100, 1000, 100)
hyper = {'n_estimators' : n_estimators}
gd = GridSearchCV(estimator = RandomForestClassifier(random_state=0), param_grid=hyper, verbose=True)
gd.fit(X, Y)
print(gd.best_score_)
print(gd.best_estimator_)
```

    Fitting 5 folds for each of 9 candidates, totalling 45 fits
    0.8193270981106021
    RandomForestClassifier(n_estimators=400, random_state=0)
    

- SVM 82.82%로 C=0.6, gamma=0.1 일 때 최고 점수이다.
- Random Forest 81.93%로 n_estimators=400일 때 최고 점수이다.

### Ensemble
1) Voting Classifier
2) Bagging
3) Boosting

### Voting Classifier
- 여러 개별 모델을 결합하여 더 강력한 모델을 만드는 기법.
  1) 다양한 분류기를 생성
  2) Voting Classifier에 포함시킴
  3) 각 분류기들의 예측을 조합하여 최종 예측을 수행.
     - hard :  각 분류기의 예측 결과를 단순히 다수결로 결정
     - soft : 각 분류기의 예측 확률을 평균하여 가장 높은 확률을 가진 클래스를 선택


```python
from sklearn.ensemble import VotingClassifier
ensemble_lin_rbf = VotingClassifier(estimators=[('KNN', KNeighborsClassifier(n_neighbors=10)),
                                   ('RBF',svm.SVC(probability=True, kernel='rbf', C=0.5, gamma=0.1)),
                                   ('RFor', RandomForestClassifier(n_estimators=500, random_state=0)),
                                   ('LR', LogisticRegression(C=0.05)),
                                   ('DT',DecisionTreeClassifier(random_state=0)),
                                   ('NB', GaussianNB()),
                                   ('svm', svm.SVC(kernel='linear',probability=True))], voting='soft').fit(train_X, train_Y)
print('The accuracy for ensembled model is:', ensemble_lin_rbf.score(test_X, test_Y))
cross = cross_val_score(ensemble_lin_rbf, X, Y, cv = 10, scoring = 'accuracy')
print('The cross validated score is', cross.mean())
```

    The accuracy for ensembled model is: 0.8246268656716418
    The cross validated score is 0.8282771535580524
    

### Bagging
- Decision Tree, Random Forest와 같이 분산이 높은 모델에서 가장 잘 작동한다. 


```python
from sklearn.ensemble import BaggingClassifier
model=BaggingClassifier(base_estimator = KNeighborsClassifier(n_neighbors=3), random_state = 0, n_estimators= 700)
model.fit(train_X, train_Y)
prediction= model.predict(test_X)
print('The accuracy for bagged KNN is', metrics.accuracy_score(prediction, test_Y))
result = cross_val_score(model, X, Y, cv=10, scoring = 'accuracy')
print('The cross validated score for bagged KNN is:', result.mean())
```

    The accuracy for bagged KNN is 0.832089552238806
    The cross validated score for bagged KNN is: 0.8104244694132333
    

### Boosting
- 전체 데이터셋에 대해 모델을 학습함. 잘못 예측된 인스턴스에 더 높은 가중치를 부여.


```python
import xgboost as xg
xgboost=xg.XGBClassifier(n_estimators=900,learning_rate=0.1)
result=cross_val_score(xgboost,X,Y,cv=10,scoring='accuracy')
print('The cross validated score for XGBoost is:',result.mean())
```

    The cross validated score for XGBoost is: 0.8160299625468165
    


```python
from sklearn.ensemble import AdaBoostClassifier
ada=AdaBoostClassifier(n_estimators=200, random_state=0, learning_rate=0.1)
result = cross_val_score(ada, X, Y, cv=10, scoring='accuracy')
print('The cross validated score for AdaBoost is:', result.mean())
```

    The cross validated score for AdaBoost is: 0.8249188514357055
    

### Confusion matrix for the Best Model


```python
ada = AdaBoostClassifier(n_estimators=200, random_state=0, learning_rate=0.05)
result=cross_val_predict(ada, X, Y, cv=10)
sns.heatmap(confusion_matrix(Y, result), cmap='winter', annot=True, fmt='2.0f')
plt.show()
```

![image](https://github.com/liayeoni/study/assets/154586550/a66241c8-857b-438e-bf76-75a1eb2abf8d)
    


### Feature Importance


```python
import xgboost as xg
from sklearn.ensemble import GradientBoostingClassifier
f, ax = plt.subplots(2, 2, figsize=(15, 12))

model = RandomForestClassifier(n_estimators=500, random_state=0)
model.fit(X, Y)
pd.Series(model.feature_importances_, X.columns).sort_values(ascending=True).plot.barh(width=0.8, ax=ax[0, 0])
ax[0, 0].set_title('Feature Importance in Random Forests')

model = AdaBoostClassifier(n_estimators=200, learning_rate=0.05, random_state=0)
model.fit(X, Y)
pd.Series(model.feature_importances_,X.columns).sort_values(ascending=True).plot.barh(width=0.8, ax=ax[0, 1], color='#ddff11')
ax[0, 1].set_title('Feature Importance in AdaBoost')

model = GradientBoostingClassifier(n_estimators=500, learning_rate=0.1, random_state=0)
model.fit(X, Y)
pd.Series(model.feature_importances_,X.columns).sort_values(ascending=True).plot.barh(width=0.8, ax=ax[1, 0], color='#FF3399')
ax[1, 0].set_title('Feature Importance in Gradient Boosting')

model = xg.XGBClassifier(n_estimators=900, learning_rate=0.1)
model.fit(X, Y)
pd.Series(model.feature_importances_,X.columns).sort_values(ascending=True).plot.barh(width=0.8, ax=ax[1, 1], color='#FD0F00')
ax[1, 1].set_title('Feature Importance in XgBoost')
plt.show
```

![image](https://github.com/liayeoni/study/assets/154586550/dc80b3cf-e9fd-4346-81d0-d39cfa33fe9a)
    


- 공통된 중요한 feature들을 확인할 수 있다. Initial, Pclass, Fare_cat
- Sex는 RandomForest에서만 중요함을 확인할 수 있다.
